 Dataflow Framework – Abstraction Level 7

## 📁 Directory Structure

```
abstraction_level_7/
│
├── main.py                         # Entry point to run the pipeline
├── dag_config.yaml                # DAG configuration (node sequence)
├── input.txt                      # Input data (logs or lines)
├── out.txt                        # Output data (generated by pipeline)
│
├── utils/
│   ├── dag_runner.py              # Core logic to run the DAG pipeline
│   └── processors/                # Processor modules (each defines a transformation)
│       ├── __init__.py
│       ├── start.py               # Start node logic
│       ├── tagger.py              # Adds tags to the data
│       ├── counter.py             # Adds running count
│       └── error_prone.py         # Demonstrates a fan-out/fan-in or stateful behavior
│
└── dashboard/
    ├── server.py                  # Optional dashboard server (can be a mock or real UI)
```

---

## 🚀 How It Works

This project is a **stream-based, DAG-driven text processor**, where each node in the DAG modifies or routes the input text data.

### Data Flow:
Each line in `input.txt` is:
1. Read and sent to the first node in the DAG.
2. Passed through processors defined in `dag_config.yaml`.
3. Modified as per processor logic.
4. Written to `out.txt` as final output.

### DAG Nodes:
Defined in `dag_config.yaml`, for example:

```yaml
start:
  next: tagger

tagger:
  next: counter

counter:
  next: error_prone

error_prone:
  next: end

end: {}
```

---

## ⚙️ Processors (Line Processors)

Each processor:
- Inherits from a common interface.
- Accepts an `Iterator[str]` and returns an `Iterator[str]`.

### Examples:

- `start.py`: Initial entry point.
- `tagger.py`: Adds `[info]` tag to each line.
- `counter.py`: Appends `[count=N]` to each line, incrementing for each message.
- `error_prone.py`: Appends `[trace=error_prone]` to simulate tracing or fan-out behavior.

---

## 🖥️ How to Run

1. Make sure your working directory is the root of the project.
2. Ensure `input.txt` has some lines of data, e.g.:

```
error: failed to start service
warn: high memory usage detected
system check complete
```

3. Run the pipeline:
```bash
python abstraction_level_7/main.py --trace
```

4. Check the output:
```bash
cat abstraction_level_7/out.txt
```

---

## 🧪 Example Output (`out.txt`)

```
[info] error: failed to start service [count=1]
[info] error: failed to start service [trace=error_prone]
[info] warn: high memory usage detected [count=2]
[info] warn: high memory usage detected [trace=error_prone]
...
```

---

## 🛠️ Features

✅ Modular processors  
✅ Stream-based processing (`Iterator[str] -> Iterator[str]`)  
✅ Fan-out/fan-in logic supported  
✅ DAG-configurable pipeline  
✅ Trace/debug mode  
✅ Optional dashboard server

---

## 🔧 Development Tips

- Add new processors in `utils/processors/`.
- Register them in `dag_runner.py`.
- Update the DAG in `dag_config.yaml`.
- Run `main.py` with `--trace` to debug the flow.
