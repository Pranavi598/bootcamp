 Dataflow Framework â€“ Abstraction Level 7

## ðŸ“ Directory Structure

```
abstraction_level_7/
â”‚
â”œâ”€â”€ main.py                         # Entry point to run the pipeline
â”œâ”€â”€ dag_config.yaml                # DAG configuration (node sequence)
â”œâ”€â”€ input.txt                      # Input data (logs or lines)
â”œâ”€â”€ out.txt                        # Output data (generated by pipeline)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dag_runner.py              # Core logic to run the DAG pipeline
â”‚   â””â”€â”€ processors/                # Processor modules (each defines a transformation)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ start.py               # Start node logic
â”‚       â”œâ”€â”€ tagger.py              # Adds tags to the data
â”‚       â”œâ”€â”€ counter.py             # Adds running count
â”‚       â””â”€â”€ error_prone.py         # Demonstrates a fan-out/fan-in or stateful behavior
â”‚
â””â”€â”€ dashboard/
    â”œâ”€â”€ server.py                  # Optional dashboard server (can be a mock or real UI)
```

---

## ðŸš€ How It Works

This project is a **stream-based, DAG-driven text processor**, where each node in the DAG modifies or routes the input text data.

### Data Flow:
Each line in `input.txt` is:
1. Read and sent to the first node in the DAG.
2. Passed through processors defined in `dag_config.yaml`.
3. Modified as per processor logic.
4. Written to `out.txt` as final output.

### DAG Nodes:
Defined in `dag_config.yaml`, for example:

```yaml
start:
  next: tagger

tagger:
  next: counter

counter:
  next: error_prone

error_prone:
  next: end

end: {}
```

---

## âš™ï¸ Processors (Line Processors)

Each processor:
- Inherits from a common interface.
- Accepts an `Iterator[str]` and returns an `Iterator[str]`.

### Examples:

- `start.py`: Initial entry point.
- `tagger.py`: Adds `[info]` tag to each line.
- `counter.py`: Appends `[count=N]` to each line, incrementing for each message.
- `error_prone.py`: Appends `[trace=error_prone]` to simulate tracing or fan-out behavior.

---

## ðŸ–¥ï¸ How to Run

1. Make sure your working directory is the root of the project.
2. Ensure `input.txt` has some lines of data, e.g.:

```
error: failed to start service
warn: high memory usage detected
system check complete
```

3. Run the pipeline:
```bash
python abstraction_level_7/main.py --trace
```

4. Check the output:
```bash
cat abstraction_level_7/out.txt
```

---

## ðŸ§ª Example Output (`out.txt`)

```
[info] error: failed to start service [count=1]
[info] error: failed to start service [trace=error_prone]
[info] warn: high memory usage detected [count=2]
[info] warn: high memory usage detected [trace=error_prone]
...
```

---

## ðŸ› ï¸ Features

âœ… Modular processors  
âœ… Stream-based processing (`Iterator[str] -> Iterator[str]`)  
âœ… Fan-out/fan-in logic supported  
âœ… DAG-configurable pipeline  
âœ… Trace/debug mode  
âœ… Optional dashboard server

---

## ðŸ”§ Development Tips

- Add new processors in `utils/processors/`.
- Register them in `dag_runner.py`.
- Update the DAG in `dag_config.yaml`.
- Run `main.py` with `--trace` to debug the flow.
